2023-01-03T10:33:16 | INFO | mmf : Logging to: ../exp/checkpoints_10Epoch_w_act_exp/Prediction_GT_Sem/train.log
2023-01-03T10:33:16 | INFO | mmf_cli.run : Namespace(config_override=None, local_rank=None, opts=['config=projects/nssr/atm/defaults.yaml', 'datasets=star_nssr', 'model=action_transition_model', 'run_type=train', 'dataset_config.star_nssr.qtype=Prediction', 'training.evaluation_interval=500', 'training.batch_size=64', 'training.max_epochs=80', 'training.checkpoint_interval=500', 'env.save_dir=../exp/checkpoints_10Epoch_w_act_exp/Prediction_GT_Sem', 'dataset_config.star_nssr.train_graph=GT', 'dataset_config.star_nssr.val_graph=Swin_STTrans_10Epoch', 'dataset_config.star_nssr.symbolic=False', 'dataset_config.star_nssr.semantic=True', 'dataset_config.star_nssr.visual=False', 'dataset_config.star_nssr.graph_type=SGDet'])
2023-01-03T10:33:16 | INFO | mmf_cli.run : Torch version: 1.6.0a0+445c276
2023-01-03T10:33:16 | INFO | mmf.utils.general : CUDA Device 0 is: Tesla V100-SXM2-32GB
2023-01-03T10:33:16 | INFO | mmf_cli.run : Using seed 20496257
2023-01-03T10:33:16 | INFO | mmf.trainers.mmf_trainer : Loading datasets
2023-01-03T10:33:21 | WARNING | py.warnings : /nobackup/users/bowu/code/STAR_code/STAR_Reasoning/NS-SR/mmf/mmf/utils/distributed.py:272: UserWarning: No 'max_length' parameter in Processor's configuration. Setting to 50.
  builtin_warn(*args, **kwargs)

2023-01-03T10:33:21 | WARNING | py.warnings : /nobackup/users/bowu/code/STAR_code/STAR_Reasoning/NS-SR/mmf/mmf/utils/distributed.py:272: UserWarning: No 'max_length' parameter in Processor's configuration. Setting to 50.
  builtin_warn(*args, **kwargs)

2023-01-03T10:33:23 | INFO | mmf.trainers.mmf_trainer : Loading model
2023-01-03T10:33:24 | WARNING | py.warnings : /nobackup/users/bowu/code/STAR_code/STAR_Reasoning/NS-SR/mmf/mmf/utils/distributed.py:272: UserWarning: No losses are defined in model configuration. You are expected to return loss in your return dict from forward.
  builtin_warn(*args, **kwargs)

2023-01-03T10:33:24 | WARNING | py.warnings : /nobackup/users/bowu/code/STAR_code/STAR_Reasoning/NS-SR/mmf/mmf/utils/distributed.py:272: UserWarning: No losses are defined in model configuration. You are expected to return loss in your return dict from forward.
  builtin_warn(*args, **kwargs)

2023-01-03T10:33:24 | INFO | mmf.trainers.mmf_trainer : Loading optimizer
2023-01-03T10:33:24 | INFO | mmf.trainers.mmf_trainer : Loading metrics
2023-01-03T10:33:24 | WARNING | py.warnings : /nobackup/users/bowu/code/STAR_code/STAR_Reasoning/NS-SR/mmf/mmf/utils/distributed.py:272: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'
  builtin_warn(*args, **kwargs)

2023-01-03T10:33:24 | WARNING | py.warnings : /nobackup/users/bowu/code/STAR_code/STAR_Reasoning/NS-SR/mmf/mmf/utils/distributed.py:272: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'
  builtin_warn(*args, **kwargs)

2023-01-03T10:33:24 | WARNING | py.warnings : /nobackup/users/bowu/code/STAR_code/STAR_Reasoning/NS-SR/mmf/mmf/utils/distributed.py:272: UserWarning: scheduler attributes has no params defined, defaulting to {}.
  builtin_warn(*args, **kwargs)

2023-01-03T10:33:24 | WARNING | py.warnings : /nobackup/users/bowu/code/STAR_code/STAR_Reasoning/NS-SR/mmf/mmf/utils/distributed.py:272: UserWarning: scheduler attributes has no params defined, defaulting to {}.
  builtin_warn(*args, **kwargs)

2023-01-03T10:33:24 | INFO | mmf.trainers.mmf_trainer : ===== Model =====
2023-01-03T10:33:24 | WARNING | py.warnings : /nobackup/users/bowu/code/STAR_code/STAR_Reasoning/NS-SR/mmf/mmf/utils/distributed.py:272: UserWarning: Both max_updates and max_epochs are specified. Favoring max_epochs: 80
  builtin_warn(*args, **kwargs)

2023-01-03T10:33:24 | WARNING | py.warnings : /nobackup/users/bowu/code/STAR_code/STAR_Reasoning/NS-SR/mmf/mmf/utils/distributed.py:272: UserWarning: Both max_updates and max_epochs are specified. Favoring max_epochs: 80
  builtin_warn(*args, **kwargs)

2023-01-03T10:33:24 | INFO | mmf.trainers.core.training_loop : Starting training...
2023-01-03T10:33:24 | INFO | torchtext.vocab : Loading vectors from /home/bowu/.cache/torch/mmf/glove.6B.300d.txt.pt
2023-01-03T10:33:25 | INFO | torchtext.vocab : Loading vectors from /home/bowu/.cache/torch/mmf/glove.6B.300d.txt.pt
2023-01-03T10:33:40 | WARNING | py.warnings : /nobackup/users/bowu/code/STAR_code/STAR_Reasoning/NS-SR/mmf/mmf/utils/distributed.py:272: UserWarning: 'losses' already present in model output. No calculation will be done in base model.
  builtin_warn(*args, **kwargs)

2023-01-03T10:33:40 | WARNING | py.warnings : /nobackup/users/bowu/code/STAR_code/STAR_Reasoning/NS-SR/mmf/mmf/utils/distributed.py:272: UserWarning: 'losses' already present in model output. No calculation will be done in base model.
  builtin_warn(*args, **kwargs)

2023-01-03T10:33:41 | WARNING | py.warnings : /nobackup/users/bowu/anaconda3/envs/mmf/lib/python3.6/site-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /home/florin/anaconda3/conda-bld/pytorch-base_1604496080370/work/torch/csrc/utils/python_arg_parser.cpp:766.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)

2023-01-03T10:33:41 | WARNING | py.warnings : /nobackup/users/bowu/anaconda3/envs/mmf/lib/python3.6/site-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  /home/florin/anaconda3/conda-bld/pytorch-base_1604496080370/work/torch/csrc/utils/python_arg_parser.cpp:766.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)

