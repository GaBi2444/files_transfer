model_config:
  visual_bert:
    losses:
    - type: cross_entropy

    bert_model_name: None
    training_head_type: classification
    type_vocab_size: 4
    hidden_size: 512
    vocab_size: 150
    num_hidden_layers: 6
    num_attention_heads: 8
    max_img_len: ${dataset_config.actionqa.max_img_seq_len}
    max_txt_len: ${dataset_config.actionqa.max_txt_seq_len}

    visual_embedding_dim: 2048
    special_visual_initialize: false
    embedding_strategy: plain
    bypass_transformer: false
    output_attentions: false
    output_hidden_states: false
    random_initialize: false
    freeze_base: false
    finetune_lr_multiplier: 1
    num_labels: 2
    # Default points to BERT pooler strategy which is to take
    # representation of CLS token after passing it through a dense layer
    pooler_strategy: default
