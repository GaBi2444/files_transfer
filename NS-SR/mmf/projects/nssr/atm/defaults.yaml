
optimizer:
  type: adam_w
  params:
    lr: 1e-4
    eps: 1e-8

evaluation:
  metrics:
  - mask_act_acc
  - mask_obj1_acc
  - mask_rel_acc
  - mask_obj2_acc

training:
  experiment_name: action_transition_model
  #max_epochs: 50
  batch_size: 2
  checkpoint_interval: 10
  evaluation_interval: 5
  log_interval: 100
  num_workers: 4
  lr_scheduler: true
  early_stop:
    criteria: star/mask_act_acc
    minimize: false

distributed:
    ###
    # Typically tcp://hostname:port that will be used to establish initial connection
    # init_method: tcp://9.47.194.162:2234
    # Rank of the current worker
    # rank: 0
    # Port number, not required if using init_method,
    port: -1
    # Backend for distributed setup
    backend: nccl
    # Total number of GPUs across all nodes (default: all visible GPUs)
    # world_size: 4 #${device_count:}
    # Set if you do not want spawn multiple processes even if
    # multiple GPUs are visible
    no_spawn: false
